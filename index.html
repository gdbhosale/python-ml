<!doctype html>
<html lang="en">

<head>
	<meta charset="utf-8">

	<title>Machine Learning using Python</title>

	<meta name="description" content="Introduction to Machine Learning using Python">
	<meta name="author" content="Dwij IT Solutions / Ganesh Bhosale">
	
	<meta property="og:url"           content="" />
    <meta property="og:type"          content="website" />
    <meta property="og:title"         content="Machine Learning using Python" />
    <meta property="og:description"   content="Introduction to Machine Learning using Python" />
    <meta property="og:image"         content="" />

	<meta name="apple-mobile-web-app-capable" content="yes" />
	<meta name="apple-mobile-web-app-status-bar-style" content="black-translucent" />

	<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">

	<!-- help : http://lab.hakim.se/reveal-js -->
	<link rel="stylesheet" href="css/reveal.css">
	<link rel="stylesheet" href="css/theme/black.css" id="theme">
	
	<link rel="stylesheet" href="css/font-awesome.min.css">

	<!-- Code syntax highlighting -->
	<link rel="stylesheet" href="lib/css/zenburn.css">

	<!-- Printing and PDF exports -->
	<script>
		var link = document.createElement( 'link' );
		link.rel = 'stylesheet';
		link.type = 'text/css';
		link.href = window.location.search.match( /print-pdf/gi ) ? 'css/print/pdf.css' : 'css/print/paper.css';
		document.getElementsByTagName( 'head' )[0].appendChild( link );
	</script>

	<!--[if lt IE 9]>
	<script src="lib/js/html5shiv.js"></script>
	<![endif]-->
</head>

<body>
<div id="fb-root"></div>
<script>(function(d, s, id) {
  var js, fjs = d.getElementsByTagName(s)[0];
  if (d.getElementById(id)) return;
  js = d.createElement(s); js.id = id;
  js.src = "//connect.facebook.net/en_US/sdk.js#xfbml=1&version=v2.5&appId=1684314365174532";
  fjs.parentNode.insertBefore(js, fjs);
}(document, 'script', 'facebook-jssdk'));</script>

	<div class="reveal">

		<!-- Any section element inside of this container is displayed as a slide -->
		<div class="slides" id="slidesD">
			<section>
				<h1>Machine Learning</h1>
				<h3>Using Python</h3>
				<p>
					<small>Created by <a target="_blank" href="https://twitter.com/gdbhosale">Ganesh Bhosale</a> in <a target="_blank" href="http://dwijitsolutions.com">Dwij IT Solutions</a></small><br>
					<a href="http://dwijitsolutions.com" target="_blank">
						<img src="images/dwij-it-solutions.png" alt="Dwij IT Solutions Logo">
					</a>
				</p>
			</section>
			
			<section>
				<h2>Your support</h2>
				<p>
					Like this <a class="cred">Machine Learning</a> tutorial, our tutorials are always open for learners. Like our facebook page to get more free tutorial updates.
				</p>
				<p>
					<div class="fb-page" data-href="https://www.facebook.com/dwijitsolutions" data-width="330" data-small-header="false" data-adapt-container-width="true" data-hide-cover="false" data-show-facepile="true"><div class="fb-xfbml-parse-ignore"><blockquote cite="https://www.facebook.com/dwijitsolutions"><a href="https://www.facebook.com/dwijitsolutions">Dwij IT Solutions</a></blockquote></div></div>
					<br><br><br>
				</p>
			</section>

			<section>
				<h2>Machine learning</h2>
				<p>
					<a class="cred">Machine learning</a> is a field of computer science that gives computers the ability to learn without being explicitly programmed.
				</p>
				<p>
					<a class="cred">Arthur Samuel</a>, an American pioneer in the field of computer gaming and artificial intelligence, coined the term "Machine Learning" in 1959 while at IBM.
				</p>
			</section>

			<section>
				<h2>Machine learning</h2>
				<p>
					It sits at the intersection of <a class="cred">statistics</a> and <a class="cred">computer science</a>, yet it can wear many different masks. You may also hear it labeled several other names or buzz words
				</p>
				<p>
					Data Science, Big Data, Artificial Intelligence, Predictive Analytics, Computational Statistics, Data Mining
				</p>
			</section>

			<section>
				<h2>Machine learning examples</h2>
				<ul class="left-align" style="font-size:33px;">
					<li>
						<a class="cred">Supervised Learning</a> - Your email provider kindly places that sketchy email from the "Nigerian prince with $50,000 to deposit into an overseas bank account" into the spam folder.
					</li>
					<li>
						<a class="cred">Unsupervised Learning</a> - Marketing firms "kindly" use hundreds of behavior and demographic indicators to segment customers into targeted offer groups. (find hidden patterns or grouping in data without labeled responses)
					</li>
					<li>
						<a class="cred">Reinforcement Learning</a> - A computer and camera within a self-driving car interact with the road and other cars to learn how to navigate a city.
					</li>
				</ul>
			</section>

			<section>
				<h2>Why Machine learning ?</h2>
				<ul class="left-align" style="font-size:33px;">
					<li>
						<a class="cred">Massive Global Demand</a> - Data scientists, software engineers, and business analysts all benefit by knowing machine learning
					</li>
					<li>
						<a class="cred">Data is Power</a> - Data is transforming everything we do. All organizations, from startups to tech giants to Fortune 500 corporations, are racing to harness their data
					</li>
					<li>
						<a class="cred">Easy Decision Making</a> - Large organizations can take fast decision based on Analysis tools powered by ML.
					</li>
				</ul>
			</section>

			<section>
				<h2>Prerequisites</h2>
				<ul class="left-align" style="font-size:33px;">
					<li>
						<a class="cred">Python for Data Science</a> - You can’t use machine learning unless you know how to program.
					</li>
					<li>
						<a class="cred">Statistics for Data Science</a> - Understanding statistics, especially Bayesian probability, is essential for many machine learning algorithms.
					</li>
					<li>
						<a class="cred">Math for Data Science</a> - Original algorithm research requires a foundation in linear algebra and multivariable calculus.
					</li>
				</ul>
			</section>

			<section>
				<h2>Requirements</h2>
				<ul class="left-align" style="font-size:33px;">
					<li>
						<a class="cred">Python 2.7</a>
					</li>
					<li>
						<a class="cred">NumPy</a> - for more efficient numerical computation
					</li>
					<!-- <li>
						<a class="cred">Pandas</a> - convenient library that supports dataframes (Optional)
					</li> -->
					<li>
						<a class="cred">Scikit-Learn / sklearn</a> - Tool for data mining
					</li>
				</ul>
			</section>

			<section>
				<h2>NumPy</h2>
				<ul class="left-align">
					<li>Mathematical and logical operations on arrays.</li>
					<li>Fourier transforms and routines for shape manipulation</li>
					<li>Operations related to linear algebra. NumPy has in-built functions for linear algebra and random number generation.</li>
				</ul><br><br>
				<a href="http://numpy.org" target="_blank">numpy.org</a>
			</section>

			<section>
				<h2>NumPy Installation</h2>
				<pre><code>pip install numpy</code></pre>
			</section>

			<section>
				<h2>NumPy Ndarray Object</h2>
				<ul class="left-align">
					<li>N-dimensional array type </li>
					<li>Every item with same size of memory</li>
					<li>Any item extracted from ndarray object (by slicing) is represented by a Python object of one of array scalar types.</li>
				</ul>
				<img src="images/ndarray.jpg" width="700">
			</section>

			<section>
				<h2>NumPy Ndarray Object</h2>
				<pre class=""><code data-trim>
numpy.array(object, dtype = None, copy = True, 
		   order = None, subok = False, ndmin = 0)
				</code></pre>
				<pre class=""><code data-trim>
>>> import numpy as np
>>> a = np.array([[1, 2], [3, 4]]) 
>>> print a
[[1 2]
 [3 4]]
>>> a = np.array([1, 2, 3], dtype = complex) 
>>> print a
[ 1.+0.j  2.+0.j  3.+0.j]
				</code></pre>
			</section>

			<section>
				<h2>NumPy Data Types</h2>
				<pre class=""><code data-trim>
bool_, int_, intc, intp, int8, int16, int32, int64,
uint8, uint16, uint32, uint64,
float_, float16, float32, float64,
complex_, complex64, complex128
				</code></pre>
			</section>

			<section>
				<h2>NumPy Array Attributes</h2>
				<pre class=""><code data-trim>
>>> import numpy as np 
>>> a = np.array([[1,2,3],[4,5,6]]) 
>>> print a.shape
(2, 3)
>>> a.shape = (3,2)
>>> print a 
[[1 2]
 [3 4]
 [5 6]]
>>> b = a.reshape(3,2)
>>> print b
[[1 2]
 [3 4]
 [5 6]]					
				</code></pre>
			</section>

			<section>
				<h2>NumPy Array Attributes</h2>
				<pre class=""><code data-trim>
>>> import numpy as np
>>> a = np.arange(24) 
>>> print a
[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23]
>>> a.ndim
1
>>> b = a.reshape(2,4,3) 
>>> print b 
[[[ 0  1  2]
  [ 3  4  5]
  [ 6  7  8]
  [ 9 10 11]]
 [[12 13 14]
  [15 16 17]
  [18 19 20]
  [21 22 23]]]
>>> a.itemsize
8			
				</code></pre>
			</section>

			<section>
				<h2>NumPy Array Creation Routines</h2>
				<pre class=""><code data-trim>
>>> # numpy.empty(shape, dtype = float, order = 'C')
>>> import numpy as np 
>>> x = np.empty([3, 2], dtype = int) 
>>> print x
[[-6917529027641081856 -6917529027641081856]
 [     140512543637508      140512543686784]
 [     140512543688528      140512543686784]]
>>> 
>>> x = np.zeros(5) 
>>> print x
[ 0.  0.  0.  0.  0.]
>>> # numpy.zeros(shape, dtype = float, order = 'C')
>>> x = np.zeros((2, 2), dtype = np.int)
>>> print x
[[0 0]
 [0 0]]
>>> # numpy.ones(shape, dtype = None, order = 'C')
>>> x = np.ones([2, 2], dtype = int) 
>>> print x
[[1 1]
 [1 1]]
>>> 
				</code></pre>
			</section>

			<section>
				<h2>NumPy - Array From Existing Data</h2>
				<pre class=""><code data-trim>
>>> # numpy.asarray(a, dtype = None, order = None)
>>> import numpy as np 
>>> x = [1,2,3] 
>>> a = np.asarray(x) 
>>> print a
[1 2 3]
>>> 
>>> x = [(1,2,3),(4,5)] 
>>> a = np.asarray(x) 
>>> print a
[(1, 2, 3) (4, 5)]
				</code></pre>
			</section>

			<section>
				<h2>NumPy - Array From Numerical Ranges</h2>
				<pre class=""><code data-trim>
>>> # numpy.arange(start, stop, step, dtype)
>>> import numpy as np 
>>> x = [1,2,3] 
>>> a = np.asarray(x) 
>>> print a
[1 2 3]
>>> 
>>> x = [(1,2,3),(4,5)] 
>>> a = np.asarray(x) 
>>> print a
[(1, 2, 3) (4, 5)]
				</code></pre>
			</section>

			<section>
				<h2>NumPy - Advanced Indexing</h2>
				<pre class=""><code data-trim>
>>> import numpy as np 
>>> x = np.array([[1, 2], [3, 4], [5, 6]]) 
>>> y = x[[0,1,2], [0,1,0]]
>>> print y
[1 4 5]
>>> 
>>> x = np.array([[ 0,  1,  2],[ 3,  4,  5],[ 6,  7,  8],[ 9, 10, 11]])
>>> print x
[[ 0  1  2]
 [ 3  4  5]
 [ 6  7  8]
 [ 9 10 11]]
>>> rows = np.array([[0,0],[3,3]])
>>> cols = np.array([[0,2],[0,2]]) 
>>> y = x[rows,cols]
>>> print y
[[ 0  2]
 [ 9 11]]
				</code></pre>
			</section>

			<section>
				<h2>NumPy - Broadcasting</h2>
				<pre class=""><code data-trim>
>>> import numpy as np 
>>> a = np.array([[0.0,0.0,0.0],[10.0,10.0,10.0],[20.0,20.0,20.0],[30.0,30.0,30.0]]) 
>>> b = np.array([1.0,2.0,3.0]) 
>>> print a
[[  0.   0.   0.]
 [ 10.  10.  10.]
 [ 20.  20.  20.]
 [ 30.  30.  30.]]
>>> print b
[ 1.  2.  3.]
>>> print a + b
[[  1.   2.   3.]
 [ 11.  12.  13.]
 [ 21.  22.  23.]
 [ 31.  32.  33.]]
				</code></pre>
			</section>

			<section>
				<h2>NumPy - Broadcasting</h2>
				<img src="images/array.jpg" width="700">
			</section>

			<section>
				<h2>NumPy - Iterating Over Array</h2>
				<pre class=""><code data-trim>
>>> import numpy as np 
>>> a = np.arange(0,60,5)
>>> a = a.reshape(3,4)
>>> print a
[[ 0  5 10 15]
 [20 25 30 35]
 [40 45 50 55]]
>>> for x in np.nditer(a):
...     print x,
... 
0 5 10 15 20 25 30 35 40 45 50 55
>>> 
				</code></pre>
			</section>

			<section>
				<h2>NumPy - Iterating Over Array</h2>
				<pre class=""><code data-trim>
>>> import numpy as np
>>> a = np.arange(0,60,5) 
>>> a = a.reshape(3,4)
>>> print a 
[[ 0  5 10 15]
 [20 25 30 35]
 [40 45 50 55]]
>>> b = a.T # Transpose of array
>>> print b
[[ 0 20 40]
 [ 5 25 45]
 [10 30 50]
 [15 35 55]]
>>> for x in np.nditer(b):
...     print x,
... 
0 5 10 15 20 25 30 35 40 45 50 55
>>> 

				</code></pre>
			</section>

			<section>
				<h2>NumPy - Modifying Array Values</h2>
				<pre class=""><code data-trim>
>>> import numpy as np
>>> a = np.arange(0,60,5)
>>> a = a.reshape(3,4)
>>> print a
[[ 0  5 10 15]
 [20 25 30 35]
 [40 45 50 55]]
>>> for x in np.nditer(a, op_flags=['readwrite']):
...     x[...]=2*x
... 
>>> print a
[[  0  10  20  30]
 [ 40  50  60  70]
 [ 80  90 100 110]]
>>> 
				</code></pre>
			</section>

			<section>
				<h2>More NUMPY</h2>
				<ul class="left-align">
					<li><a href="https://www.tutorialspoint.com/numpy/numpy_array_manipulation.htm" target="_blank">Array Manipulation</a></li>
					<li><a href="https://www.tutorialspoint.com/numpy/numpy_binary_operators.htm" target="_blank">Binary Operators</a></li>
					<li><a href="https://www.tutorialspoint.com/numpy/numpy_mathematical_functions.htm" target="_blank">Mathematical Functions</a></li>
					<li><a href="https://www.tutorialspoint.com/numpy/numpy_arithmetic_operations.htm" target="_blank">Arithmetic Operations</a></li>
					<li><a href="https://www.tutorialspoint.com/numpy/numpy_statistical_functions.htm" target="_blank">Statistical Functions</a></li>
					<li><a href="https://www.tutorialspoint.com/numpy/numpy_matplotlib.htm" target="_blank">Matplotlib</a></li>
				</ul><br><br>
				
			</section>

			<section>
				<h2>Scikit Learn</h2>
				<ul class="left-align">
					<li>Simple and efficient tools for data mining and data analysis</li>
					<li>Accessible to everybody, and reusable in various contexts</li>
					<li>Built on NumPy, SciPy, and matplotlib</li>
					<li>Open source, commercially usable - BSD license</li>
				</ul><br><br>
				<a href="http://scikit-learn.org/stable/" target="_blank">http://scikit-learn.org</a>
			</section>

			<section>
				<h2>Scikit Installation</h2>
				<pre><code>sudo apt-get install python-pip
sudo apt-get install python-scipy
sudo pip install -U scikit-learn</code></pre>
			</section>

			<section data-background="#F56767" data-background-transition="zoom" data-transition="zoom">
				<h2>Support Vector Machines (SVM)</h2>
				<p>
					In Machine learning, support vector machines (SVMs) are supervised learning models with associated learning algorithms that analyze data used for classification and regression analysis.
				</p>
			</section>

			<section>
				<h2>SVM Advantages</h2>
				<ul class="left-align">
					<li>Effective in high dimensional spaces</li>
					<li>Uses a subset of training points in the decision function (mory efficient support vectors)</li>
					<li>Versatile: different Kernel functions can be specified for the decision function</li>
					<li>Common kernels are provided, but it is also possible to specify custom kernels</li>
				</ul>
			</section>

			<section>
				<h2>SVM Disadvantages</h2>
				<ul class="left-align">
					<li>If the number of features is much greater than the number of samples, avoid over-fitting in choosing Kernel functions and regularization term is crucial.</li>
					<li>SVMs do not directly provide probability estimates, these are calculated using an expensive five-fold cross-validation.</li>
				</ul>
			</section>

			<section>
				<h2>SVM</h2>
				<ul class="left-align">
					<li>Supports both dense (numpy.ndarray) & sparse (scipy.sparse) Data</li>
					<li>SVMs do not directly provide probability estimates, these are calculated using an expensive five-fold cross-validation.</li>
				</ul>
			</section>

			<section>
				<h2>What is Supervised Learning ?</h2>
				<img src="images/learning-supervised2.jpg">
			</section>

			<section>
				<h2>What is SVM ?</h2>
				<p>
					Visit a Node nearest to you only once. Also called greedy algorithm.
				</p>
				<img src="images/svm-data-points.jpg" width="400">
				<img src="images/svm-infinte-linear-separators.jpg" width="400">
			</section>

			<section>
				<h2>What is SVM ?</h2>
				<!-- https://onionesquereality.wordpress.com/2009/03/22/why-are-support-vectors-machines-called-so/ -->
				<ul class="left-align">
					<li>we see that the <b class="cred">thinner lines</b> mark the distance from the classifier to the closest data points called the support vectors.</li>
					<li>distance between the two thin lines is called the <b class="cred">margin</b></li>
				</ul>
				<img src="images/svm-optimal-margin-classifier.jpg">
			</section>

			<section>
				<h2>What is SVM ?</h2>
				<ul class="left-align">
					<li>mechanical analogy emphasizes: Most important data points are the support vectors as they have the maximum values of <b class="cred">Alpha</b> <img src="images/alpha.png">. These points exert the maximum force on the decision sheet.</li>
					<li>these specific data points are “supporting” the hyperplane into <b class="cred">equilibrium</b>.</li>
				</ul>
			</section>

			<section>
				<h2>SVM Example</h2>
				<pre class=""><code data-trim>
>>> from sklearn import svm
>>> X = [[0, 0], [1, 1]]
>>> y = [0, 1]
>>> clf = svm.SVC()
>>> clf.fit(X, y)  
SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,
    decision_function_shape='ovr', degree=3, gamma='auto', kernel='rbf',
    max_iter=-1, probability=False, random_state=None, shrinking=True,
	tol=0.001, verbose=False)
>>> 
>>> clf.predict([[2., 2.]])
array([1])
>>> # get support vectors
>>> clf.support_vectors_
array([[ 0.,  0.],
       [ 1.,  1.]])
>>> # get indices of support vectors
>>> clf.support_ 
array([0, 1]...)
>>> # get number of support vectors for each class
>>> clf.n_support_ 
array([1, 1]...)
				</code></pre>
			</section>

			<!-- http://scikit-learn.org/stable/auto_examples/index.html -->

			<section data-background="#F56767" data-background-transition="zoom" data-transition="zoom">
				<h2>Classification</h2>
				<img src="images/sphx_glr_plot_classification.jpg" width="700">
			</section>

			<section>
				<h2>Classification</h2>
				<p><b class="cred">Def:</b> Identifying to which category an object belongs to.</p>
				<p><b class="cred">Applications:</b> Spam detection, Image recognition.</p>
				<p><b class="cred">Algorithms:</b> SVM, Nearest Neighbors, Random Forest...</p>
			</section>

			<section>
				<h2>Nearest Neighbors</h2>
				<p>
					Visit a Node nearest to you only once. Also called greedy algorithm.
				</p>
				<img src="images/nearest_neighbour_algo.gif" width="400">
			</section>

			<section>
				<h2>Nearest Neighbors</h2>
				<img src="images/Nearest-Neighbor.gif" width="700">
			</section>

			<section>
				<h2>K Nearest Neighbors (kNN)</h2>
				<p>
					Visit a Node nearest to you only once. Also called greedy algorithm.
				</p>
				<img src="images/k_nearest_neighbour_algo.jpg" width="700">
			</section>

			<section>
				<h2>Nearest Neighbors</h2>
				<img src="images/Nearest-neighbor_chain_algorithm_animated.gif" width="400">
			</section>

			<section>
				<h2 class="text-normal">k Nearest Neighbors Classification</h2>
				<p>
						Classification is computed from a simple majority vote of the nearest neighbors of each point: a query point is assigned the data class which has the most representatives within the nearest neighbors of the point.
				</p>
				<img src="images/sphx_glr_plot_classification.jpg" width="900">
			</section>

			<section>
				<pre class="stretch"><code data-trim>
import numpy as np
import matplotlib.pyplot as plt
from matplotlib.colors import ListedColormap
from sklearn import neighbors, datasets

n_neighbors = 15

# import some data to play with
iris = datasets.load_iris()

# we only take the first two features. We could avoid this ugly
# slicing by using a two-dim dataset
X = iris.data[:, :2] # all rows x 2 columns
y = iris.target

h = .02  # step size in the mesh

# Create color maps
cmap_light = ListedColormap(['#FFAAAA', '#AAFFAA', '#AAAAFF'])
cmap_bold = ListedColormap(['#FF0000', '#00FF00', '#0000FF'])

for weights in ['uniform', 'distance']:
    # we create an instance of Neighbours Classifier and fit the data.
    clf = neighbors.KNeighborsClassifier(n_neighbors, weights=weights)
    clf.fit(X, y)

    # Plot the decision boundary. For that, we will assign a color to each
    # point in the mesh [x_min, x_max]x[y_min, y_max].
    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1
    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1
    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),
                np.arange(y_min, y_max, h))
    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])

    # Put the result into a color plot
    Z = Z.reshape(xx.shape)
    plt.figure()
    plt.pcolormesh(xx, yy, Z, cmap=cmap_light)

    # Plot also the training points
    plt.scatter(X[:, 0], X[:, 1], c=y, cmap=cmap_bold,
                edgecolor='k', s=20)
    plt.xlim(xx.min(), xx.max())
    plt.ylim(yy.min(), yy.max())
    plt.title("3-Class classification (k = %i, weights = '%s')"
              	% (n_neighbors, weights))

plt.show()
				</code></pre>
			</section>
			
			
			
			<section>
				<h2 class="text-normal">KNeighborsClassifier</h2>
				<pre class=""><code data-trim>
				sklearn.neighbors.KNeighborsClassifier(
	n_neighbors = 5,
	weights = ’uniform’,
	algorithm = ’auto’,
	leaf_size = 30,
	p = 2,
	metric = ’minkowski’,
	metric_params = None,
	n_jobs = 1,
	**kwargs )
				</code></pre>
			</section>

			<section>
				<h2 class="text-normal">SVM kernels</h2>
				<p>linear, rbf, poly</p>
				<img src="images/sphx_glr_plot_iris_exercise_000.png" width="300">
				<img src="images/sphx_glr_plot_iris_exercise_001.png" width="300">
				<img src="images/sphx_glr_plot_iris_exercise_002.png" width="300">
			</section>

			<section>
				<pre class="stretch"><code data-trim>
import numpy as np
import matplotlib.pyplot as plt
from sklearn import datasets, svm

iris = datasets.load_iris()
X = iris.data
y = iris.target

X = X[y != 0, :2]
y = y[y != 0]

n_sample = len(X)

np.random.seed(0)
order = np.random.permutation(n_sample)
X = X[order]
y = y[order].astype(np.float)

X_train = X[:int(.9 * n_sample)]
y_train = y[:int(.9 * n_sample)]
X_test = X[int(.9 * n_sample):]
y_test = y[int(.9 * n_sample):]

# fit the model
for fig_num, kernel in enumerate(('linear', 'rbf', 'poly')):
    clf = svm.SVC(kernel=kernel, gamma=10)
    clf.fit(X_train, y_train)

    plt.figure(fig_num)
    plt.clf()
    plt.scatter(X[:, 0], X[:, 1], c=y, zorder=10, cmap=plt.cm.Paired,
                edgecolor='k', s=20)

    # Circle out the test data
    plt.scatter(X_test[:, 0], X_test[:, 1], s=80, facecolors='none',
                zorder=10, edgecolor='k')

    plt.axis('tight')
    x_min = X[:, 0].min()
    x_max = X[:, 0].max()
    y_min = X[:, 1].min()
    y_max = X[:, 1].max()

    XX, YY = np.mgrid[x_min:x_max:200j, y_min:y_max:200j]
    Z = clf.decision_function(np.c_[XX.ravel(), YY.ravel()])

    # Put the result into a color plot
    Z = Z.reshape(XX.shape)
    plt.pcolormesh(XX, YY, Z > 0, cmap=plt.cm.Paired)
    plt.contour(XX, YY, Z, colors=['k', 'k', 'k'],
                linestyles=['--', '-', '--'], levels=[-.5, 0, .5])

    plt.title(kernel)
plt.show()
				</code></pre>
			</section>

			<section>
				<h2 class="text-normal">Classifier comparison</h2>
				<img src="images/sphx_glr_plot_classifier_comparison_001.png" width="900">
			</section>

			<section>
				<pre class="stretch"><code data-trim>
import numpy as np
import matplotlib.pyplot as plt
from matplotlib.colors import ListedColormap
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.datasets import make_moons, make_circles, make_classification
from sklearn.neural_network import MLPClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import SVC
from sklearn.gaussian_process import GaussianProcessClassifier
from sklearn.gaussian_process.kernels import RBF
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis

h = .02  # step size in the mesh

names = ["Nearest Neighbors", "Linear SVM", "RBF SVM", "Gaussian Process",
         "Decision Tree", "Random Forest", "Neural Net", "AdaBoost",
         "Naive Bayes", "QDA"]

classifiers = [
    KNeighborsClassifier(3),
    SVC(kernel="linear", C=0.025),
    SVC(gamma=2, C=1),
    GaussianProcessClassifier(1.0 * RBF(1.0)),
    DecisionTreeClassifier(max_depth=5),
    RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1),
    MLPClassifier(alpha=1),
    AdaBoostClassifier(),
    GaussianNB(),
    QuadraticDiscriminantAnalysis()]

X, y = make_classification(n_features=2, n_redundant=0, n_informative=2,
                           random_state=1, n_clusters_per_class=1)
rng = np.random.RandomState(2)
X += 2 * rng.uniform(size=X.shape)
linearly_separable = (X, y)

datasets = [make_moons(noise=0.3, random_state=0),
            make_circles(noise=0.2, factor=0.5, random_state=1),
            linearly_separable
            ]

figure = plt.figure(figsize=(27, 9))
i = 1
# iterate over datasets
for ds_cnt, ds in enumerate(datasets):
    # preprocess dataset, split into training and test part
    X, y = ds
    X = StandardScaler().fit_transform(X)
    X_train, X_test, y_train, y_test = \
        train_test_split(X, y, test_size=.4, random_state=42)

    x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5
    y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5
    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),
                         np.arange(y_min, y_max, h))

    # just plot the dataset first
    cm = plt.cm.RdBu
    cm_bright = ListedColormap(['#FF0000', '#0000FF'])
    ax = plt.subplot(len(datasets), len(classifiers) + 1, i)
    if ds_cnt == 0:
        ax.set_title("Input data")
    # Plot the training points
    ax.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=cm_bright,
               edgecolors='k')
    # and testing points
    ax.scatter(X_test[:, 0], X_test[:, 1], c=y_test, cmap=cm_bright, alpha=0.6,
               edgecolors='k')
    ax.set_xlim(xx.min(), xx.max())
    ax.set_ylim(yy.min(), yy.max())
    ax.set_xticks(())
    ax.set_yticks(())
    i += 1

    # iterate over classifiers
    for name, clf in zip(names, classifiers):
        ax = plt.subplot(len(datasets), len(classifiers) + 1, i)
        clf.fit(X_train, y_train)
        score = clf.score(X_test, y_test)

        # Plot the decision boundary. For that, we will assign a color to each
        # point in the mesh [x_min, x_max]x[y_min, y_max].
        if hasattr(clf, "decision_function"):
            Z = clf.decision_function(np.c_[xx.ravel(), yy.ravel()])
        else:
            Z = clf.predict_proba(np.c_[xx.ravel(), yy.ravel()])[:, 1]

        # Put the result into a color plot
        Z = Z.reshape(xx.shape)
        ax.contourf(xx, yy, Z, cmap=cm, alpha=.8)

        # Plot also the training points
        ax.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=cm_bright,
                   edgecolors='k')
        # and testing points
        ax.scatter(X_test[:, 0], X_test[:, 1], c=y_test, cmap=cm_bright,
                   edgecolors='k', alpha=0.6)

        ax.set_xlim(xx.min(), xx.max())
        ax.set_ylim(yy.min(), yy.max())
        ax.set_xticks(())
        ax.set_yticks(())
        if ds_cnt == 0:
            ax.set_title(name)
        ax.text(xx.max() - .3, yy.min() + .3, ('%.2f' % score).lstrip('0'),
                size=15, horizontalalignment='right')
        i += 1

plt.tight_layout()
plt.show()
				</code></pre>
			</section>

			<section>
				<h2 class="text-normal">Recognizing hand-written Digits</h2>
				<img src="images/sphx_glr_plot_digits_classification_001.png" width="700">
			</section>

			<section>
				<pre class="stretch"><code data-trim>
# Standard scientific Python imports
import matplotlib.pyplot as plt

# Import datasets, classifiers and performance metrics
from sklearn import datasets, svm, metrics

# The digits dataset
digits = datasets.load_digits()

# The data that we are interested in is made of 8x8 images of digits, let's
# have a look at the first 4 images, stored in the `images` attribute of the
# dataset.  If we were working from image files, we could load them using
# matplotlib.pyplot.imread.  Note that each image must have the same size. For these
# images, we know which digit they represent: it is given in the 'target' of
# the dataset.
images_and_labels = list(zip(digits.images, digits.target))
for index, (image, label) in enumerate(images_and_labels[:4]):
	plt.subplot(2, 4, index + 1)
	plt.axis('off')
	plt.imshow(image, cmap=plt.cm.gray_r, interpolation='nearest')
	plt.title('Training: %i' % label)

# To apply a classifier on this data, we need to flatten the image, to
# turn the data in a (samples, feature) matrix:
n_samples = len(digits.images)
data = digits.images.reshape((n_samples, -1))

# Create a classifier: a support vector classifier
classifier = svm.SVC(gamma=0.001)

# We learn the digits on the first half of the digits
classifier.fit(data[:n_samples // 2], digits.target[:n_samples // 2])

# Now predict the value of the digit on the second half:
expected = digits.target[n_samples // 2:]
predicted = classifier.predict(data[n_samples // 2:])

print("Classification report for classifier %s:\n%s\n"
		% (classifier, metrics.classification_report(expected, predicted)))
print("Confusion matrix:\n%s" % metrics.confusion_matrix(expected, predicted))

images_and_predictions = list(zip(digits.images[n_samples // 2:], predicted))
for index, (image, prediction) in enumerate(images_and_predictions[:4]):
	plt.subplot(2, 4, index + 5)
	plt.axis('off')
	plt.imshow(image, cmap=plt.cm.gray_r, interpolation='nearest')
	plt.title('Prediction: %i' % prediction)

plt.show()
				</code></pre>
			</section>

			<section>
				<h2 class="text-normal">Faces recognition using eigenfaces and SVMs</h2>
				<img src="images/sphx_glr_plot_face_recognition_001.png" width="500">
			</section>

			<section>
				<pre class="stretch"><code data-trim>
from __future__ import print_function

from time import time
import logging
import matplotlib.pyplot as plt

from sklearn.model_selection import train_test_split
from sklearn.model_selection import GridSearchCV
from sklearn.datasets import fetch_lfw_people
from sklearn.metrics import classification_report
from sklearn.metrics import confusion_matrix
from sklearn.decomposition import PCA
from sklearn.svm import SVC



# Display progress logs on stdout
logging.basicConfig(level=logging.INFO, format='%(asctime)s %(message)s')


# #############################################################################
# Download the data, if not already on disk and load it as numpy arrays

lfw_people = fetch_lfw_people(min_faces_per_person=70, resize=0.4)

# introspect the images arrays to find the shapes (for plotting)
n_samples, h, w = lfw_people.images.shape

# for machine learning we use the 2 data directly (as relative pixel
# positions info is ignored by this model)
X = lfw_people.data
n_features = X.shape[1]

# the label to predict is the id of the person
y = lfw_people.target
target_names = lfw_people.target_names
n_classes = target_names.shape[0]

print("Total dataset size:")
print("n_samples: %d" % n_samples)
print("n_features: %d" % n_features)
print("n_classes: %d" % n_classes)


# #############################################################################
# Split into a training set and a test set using a stratified k fold

# split into a training and testing set
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.25, random_state=42)


# #############################################################################
# Compute a PCA (eigenfaces) on the face dataset (treated as unlabeled
# dataset): unsupervised feature extraction / dimensionality reduction
n_components = 150

print("Extracting the top %d eigenfaces from %d faces"
      % (n_components, X_train.shape[0]))
t0 = time()
pca = PCA(n_components=n_components, svd_solver='randomized',
          whiten=True).fit(X_train)
print("done in %0.3fs" % (time() - t0))

eigenfaces = pca.components_.reshape((n_components, h, w))

print("Projecting the input data on the eigenfaces orthonormal basis")
t0 = time()
X_train_pca = pca.transform(X_train)
X_test_pca = pca.transform(X_test)
print("done in %0.3fs" % (time() - t0))


# #############################################################################
# Train a SVM classification model

print("Fitting the classifier to the training set")
t0 = time()
param_grid = {'C': [1e3, 5e3, 1e4, 5e4, 1e5],
              'gamma': [0.0001, 0.0005, 0.001, 0.005, 0.01, 0.1], }
clf = GridSearchCV(SVC(kernel='rbf', class_weight='balanced'), param_grid)
clf = clf.fit(X_train_pca, y_train)
print("done in %0.3fs" % (time() - t0))
print("Best estimator found by grid search:")
print(clf.best_estimator_)


# #############################################################################
# Quantitative evaluation of the model quality on the test set

print("Predicting people's names on the test set")
t0 = time()
y_pred = clf.predict(X_test_pca)
print("done in %0.3fs" % (time() - t0))

print(classification_report(y_test, y_pred, target_names=target_names))
print(confusion_matrix(y_test, y_pred, labels=range(n_classes)))


# #############################################################################
# Qualitative evaluation of the predictions using matplotlib

def plot_gallery(images, titles, h, w, n_row=3, n_col=4):
    """Helper function to plot a gallery of portraits"""
    plt.figure(figsize=(1.8 * n_col, 2.4 * n_row))
    plt.subplots_adjust(bottom=0, left=.01, right=.99, top=.90, hspace=.35)
    for i in range(n_row * n_col):
        plt.subplot(n_row, n_col, i + 1)
        plt.imshow(images[i].reshape((h, w)), cmap=plt.cm.gray)
        plt.title(titles[i], size=12)
        plt.xticks(())
        plt.yticks(())


# plot the result of the prediction on a portion of the test set

def title(y_pred, y_test, target_names, i):
    pred_name = target_names[y_pred[i]].rsplit(' ', 1)[-1]
    true_name = target_names[y_test[i]].rsplit(' ', 1)[-1]
    return 'predicted: %s\ntrue:      %s' % (pred_name, true_name)

prediction_titles = [title(y_pred, y_test, target_names, i)
                     for i in range(y_pred.shape[0])]

plot_gallery(X_test, prediction_titles, h, w)

# plot the gallery of the most significative eigenfaces

eigenface_titles = ["eigenface %d" % i for i in range(eigenfaces.shape[0])]
plot_gallery(eigenfaces, eigenface_titles, h, w)

plt.show()
				</code></pre>
			</section>

			<!--
			<section>
				<pre class="stretch"><code data-trim>
Classification report for classifier SVC(C=1.0, cache_size=200,
class_weight=None, coef0=0.0, decision_function_shape='ovr',
degree=3, gamma=0.001, kernel='rbf', max_iter=-1,
probability=False, random_state=None, shrinking=True, tol=0.001,
verbose=False):
			precision    recall  f1-score   support

		0       1.00      0.99      0.99        88
		1       0.99      0.97      0.98        91
		2       0.99      0.99      0.99        86
		3       0.98      0.87      0.92        91
		4       0.99      0.96      0.97        92
		5       0.95      0.97      0.96        91
		6       0.99      0.99      0.99        91
		7       0.96      0.99      0.97        89
		8       0.94      1.00      0.97        88
		9       0.93      0.98      0.95        92

avg / total      	0.97      0.97      0.97       899


Confusion matrix:
[[87  0  0  0  1  0  0  0  0  0]
 [ 0 88  1  0  0  0  0  0  1  1]
 [ 0  0 85  1  0  0  0  0  0  0]
 [ 0  0  0 79  0  3  0  4  5  0]
 [ 0  0  0  0 88  0  0  0  0  4]
 [ 0  0  0  0  0 88  1  0  0  2]
 [ 0  1  0  0  0  0 90  0  0  0]
 [ 0  0  0  0  0  1  0 88  0  0]
 [ 0  0  0  0  0  0  0  0 88  0]
 [ 0  0  0  1  0  1  0  0  0 90]]
				</code></pre>
			</section>
			-->

			<section>
				<h2>References</h2>
				<ul>
					<li><a target="_blank" href="https://www.python.org">python.org</a></li>
					<li><a target="_blank" href="http://scikit-learn.org">scikit-learn.org</a></li>
					<li><a target="_blank" href="https://www.tutorialspoint.com/numpy">tutorialspoint.com</a></li>
				</ul>
			</section>

			<section style="text-align: left;">
				<h1>Thank you</h1>
				<p>
					- <a target="_blank" href="https://ganeshbhosale.com">Ganesh Bhosale</a> <br>
					- <a target="_blank" href="http://dwijitsolutions.com">Dwij IT Solutions</a>
				</p>
				<p>Did you liked this tutorial ?</p>
				<p>
					<div class="fb-like" data-layout="button_count" data-action="like" data-show-faces="true" data-share="true"></div>
				</p>
				<p>Checkout more such tutorials on: <a href="http://dwij.net/tuts">http://dwij.net/tuts</a></p>
				<!--<div class="fb-like" data-href="https://developers.facebook.com/docs/plugins/" data-layout="button_count" data-action="like" data-show-faces="true" data-share="true"></div>-->
			</section>
		</div>
	</div>

	<script src="lib/js/head.min.js"></script>
	<script src="js/reveal.js"></script>

	<script>
		// Full list of configuration options available at:
		// https://github.com/hakimel/reveal.js#configuration
		Reveal.initialize({
			slideNumber: 'c/t',
			controls: true,
			progress: true,
			history: true,
			center: true,

			transition: 'slide', // none/fade/slide/convex/concave/zoom

			// Optional reveal.js plugins
			dependencies: [
				{ src: 'lib/js/classList.js', condition: function() { return !document.body.classList; } },
				{ src: 'plugin/markdown/marked.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },
				{ src: 'plugin/markdown/markdown.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },
				{ src: 'plugin/highlight/highlight.js', async: true, condition: function() { return !!document.querySelector( 'pre code' ); }, callback: function() { hljs.initHighlightingOnLoad(); } },
				{ src: 'plugin/zoom-js/zoom.js', async: true },
				{ src: 'plugin/notes/notes.js', async: true }
			]
		});
	</script>
	
	<div class="share-reveal">
		<a target="_blank" class="share-reveal-editor" href="http://dwijitsolutions.com">Powered by Dwij IT Solutions</a>
		<a target="_blank" class="share-reveal-facebook" href="https://www.facebook.com/DwijItSolutions">
			<svg viewBox="-8 -8 48 48" width="20" height="20" version="1.1" xmlns="http://www.w3.org/2000/svg">
				<path fill="#666666" d="M25.613-4.557c0,0-3.707,0-6.166,0c-3.662,0-7.732,1.535-7.732,6.835c0.019,1.845,0,3.613,0,5.603H7.481 v6.728h4.366v19.37h8.021V14.48h5.295l0.479-6.618h-5.913c0,0,0.016-2.946,0-3.8c0-2.093,2.184-1.974,2.312-1.974 c1.042,0,3.059,0.003,3.578,0v-6.646H25.613z"/>
			</svg>
		</a>
		<a target="_blank" class="share-reveal-facebook" href="https://twitter.com/gdbhosale">
			<svg viewBox="0 0 2000 1625.36" width="20" height="20" version="1.1" xmlns="http://www.w3.org/2000/svg">
				<path fill="#666666" d="m 1999.9999,192.4 c -73.58,32.64 -152.67,54.69 -235.66,64.61 84.7,-50.78 149.77,-131.19 180.41,-227.01 -79.29,47.03 -167.1,81.17 -260.57,99.57 C 1609.3399,49.82 1502.6999,0 1384.6799,0 c -226.6,0 -410.328,183.71 -410.328,410.31 0,32.16 3.628,63.48 10.625,93.51 -341.016,-17.11 -643.368,-180.47 -845.739,-428.72 -35.324,60.6 -55.5583,131.09 -55.5583,206.29 0,142.36 72.4373,267.95 182.5433,341.53 -67.262,-2.13 -130.535,-20.59 -185.8519,-51.32 -0.039,1.71 -0.039,3.42 -0.039,5.16 0,198.803 141.441,364.635 329.145,402.342 -34.426,9.375 -70.676,14.395 -108.098,14.395 -26.441,0 -52.145,-2.578 -77.203,-7.364 52.215,163.008 203.75,281.649 383.304,284.946 -140.429,110.062 -317.351,175.66 -509.5972,175.66 -33.1211,0 -65.7851,-1.949 -97.8828,-5.738 181.586,116.4176 397.27,184.359 628.988,184.359 754.732,0 1167.462,-625.238 1167.462,-1167.47 0,-17.79 -0.41,-35.48 -1.2,-53.08 80.1799,-57.86 149.7399,-130.12 204.7499,-212.41"></path>
			</svg>
		</a>
	</div>
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');
  ga('create', 'UA-31231580-28', 'auto');
  ga('send', 'pageview');
</script>
</body>
</html>
